{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Standardization Vs Normalization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0+qjJIwVOy6Jf1M0A2ODN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adarsharegmi/Car-body-detection-Dphi/blob/main/Data_Standardization_Vs_Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CeyjHZzw-Vb6"
      },
      "outputs": [],
      "source": [
        "from numpy import asarray\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = asarray([[100, 0.001],\n",
        "\t\t\t\t[8, 0.05],\n",
        "\t\t\t\t[50, 0.005],\n",
        "\t\t\t\t[88, 0.07],\n",
        "\t\t\t\t[4, 0.1]])"
      ],
      "metadata": {
        "id": "gezG_So6__sy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalization\n",
        "\n",
        "MinMaxScaler().fit_transform(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fedf94gVAIFW",
        "outputId": "9cad46ae-5cb3-43de-cb34-10b1694b8e30"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.        ],\n",
              "       [0.04166667, 0.49494949],\n",
              "       [0.47916667, 0.04040404],\n",
              "       [0.875     , 0.6969697 ],\n",
              "       [0.        , 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# standardization\n",
        "# sklearn\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# transform data\n",
        "scaled = scaler.fit_transform(data)\n",
        "print(scaled)\n",
        "\n",
        "\n",
        "\n",
        "# Tensorflow normalize api\n",
        "tf.keras.utils.normalize(\n",
        "    x, axis=-1, order=2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNpsmtMxAQEb",
        "outputId": "c425ce00-425d-4734-bdbe-f1d35613ea0b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.26398112 -1.16389967]\n",
            " [-1.06174414  0.12639634]\n",
            " [ 0.         -1.05856939]\n",
            " [ 0.96062565  0.65304778]\n",
            " [-1.16286263  1.44302493]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data standardization  vs Data normalization\n",
        "\n",
        "`This  totally depends upon the practice and test process. Some may be require standardization whereas some for normalization. The data if is in normal distribution better probability is to go for data standardization than normalization.`\n",
        "\n"
      ],
      "metadata": {
        "id": "ojB4fl_WC2IG"
      }
    }
  ]
}